# Задание 1 — подсчет количества уникальных IPv6-адресов

Нужно определить количество **уникальных** IPv6-адресов во входном текстовом файле. Адреса могут быть записаны в разных допустимых формах (сокращение `::`, разные регистры, ведущие нули), поэтому перед сравнением они приводятся к каноническому виду.

Решение написано **только на стандартной библиотеке Python**, устойчиво к большим объемам данных и ограничению по памяти (~1 ГБ). Используются временные файлы и распараллеливание на этапе слияния.

---
## Подготовка. Генерация тестовых данных (опционально)

Если нужно сгенерировать входной файл с IPv6-адресами, используйте скрипт генератора (`generate_data.py`).

Формат запуска:

```bash
python generate_data.py <output_file> <num_unique> <total_size> [--seed SEED]
```

### Параметры:

- output_file — путь к выходному файлу с данными (будет создан или перезаписан).

- num_unique — количество уникальных IPv6-адресов.

- total_size — общее количество строк в выходном файле (должно быть total_size >= num_unique).

- --seed — (необязательно) фиксирует генератор случайных чисел для воспроизводимости (по умолчанию 42).

### Примеры:

Сгенерировать файл из 1000 уникальных адресов и 100000 строк:
```
python generate_data.py data.txt 1000 100000
```

То же самое, но с фиксированным seed:
```
python generate_data.py data.txt 1000 100000 --seed 42
```

## Как запустить

Требуется Python 3.8+.

Запуск (строго 2 аргумента, как в условии):

```bash
python solve.py <input.txt> <output.txt>
```
Пример:
```bash
python solve.py data.txt answer.txt
```

В answer.txt будет записано одно целое число — количество уникальных IPv6-адресов.

## Нормализация IPv6 (канонический вид)

IPv6 может быть записан по-разному, но обозначать один и тот же адрес.

Чтобы сравнение было корректным, каждая строка преобразуется в каноническое бинарное представление длиной 16 байт:

- используется socket.inet_pton(socket.AF_INET6, addr);

- результат — 16 байт (128 бит), однозначный для данного IPv6;

- поэтому любые текстовые варианты одного адреса становятся одинаковыми 16 байтами.

Далее сравнение, сортировка и подсчет уникальных выполняются именно по этим 16 байтам.

## Алгоритм

В программе реализованы две стратегии (для набора баллов по критериям), и выбор делается автоматически по размеру входного файла.
### 1) Базовая стратегия: все в оперативной памяти

Для небольших входных файлов используется следующий подход:

1. Построчно читаем файл.
2. Нормализуем IPv6 в 16-байтное представление.
3. Добавляем в множество set().
4. Ответ равен размеру множества.

Этот вариант очень быстрый, но на огромных файлах может не уложиться в лимит памяти, поэтому для больших входов применяется оптимизированная стратегия.

### 2) Оптимизированная стратегия: внешняя сортировка

Для больших файлов используется внешняя сортировка (external merge sort) с временными файлами.

##### Шаг A. Формирование начальных отсортированных серий (runs)

1. Читаем входной файл потоком.
2. Преобразуем адрес в 16 байт.
3. Накапливаем в памяти блок из chunk_records адресов.
4. Сортируем блок.
5. Удаляем дубликаты внутри блока (после сортировки).
6. Записываем получившуюся отсортированную серию в бинарный файл (run).

Каждый run-файл — это последовательность записей фиксированной длины 16 байт.

##### Шаг B. Многоэтапное слияние run-файлов партиями

Нельзя открывать слишком много файлов одновременно (ограничение ОС).

Поэтому run-файлы сливаются пакетами размером не более fan_in:

1. берем группу из ≤ fan_in run-файлов;
2. выполняем k-way merge и получаем новый отсортированный run-файл;
3. старые файлы удаляем, освобождая место на диске.

Шаг повторяется несколько уровней, пока число файлов не станет достаточно маленьким для финального этапа.

##### Шаг C. Финальный k-way merge и подсчет уникальных

Когда осталось несколько отсортированных run-файлов, выполняется финальный k-way merge:

1. из каждого файла берется текущая запись (16 байт) и помещается в кучу (heapq);

2. каждый раз извлекается минимальная запись среди всех файлов;

3. уникальные считаются по условию текущая_запись != предыдущая_запись.

Отдельный итоговый файл после слияния не создается — сразу считается число уникальных значений.

### Ускорение: распараллеливание

На одном уровне многоэтапного слияния разные пакеты файлов независимы: их можно сливать одновременно.

Поэтому в функции reduce_runs() слияние пакетов запускается параллельно через multiprocessing.Pool (стандартная библиотека Python). Это ускоряет обработку больших входных данных на многоядерных CPU.

## Параметры, используемые в коде

Порог выбора стратегии: SMALL_BYTES = 200 MB
(меньше — считаем через set(), больше — внешняя сортировка).

Размер блока для формирования run-файлов: chunk_records = 1000000.

Ограничение на число файлов в одном слиянии: fan_in = 128.

## Дополнительно: вероятностные методы и другие подходы

Ниже перечислены варианты, которые рассматривались как способы ускорения или уменьшения потребления памяти.

### Вероятностные алгоритмы для оценки числа уникальных

Вместо точного подсчета храним компактную структуру, которая позволяет оценить количество уникальных значений с небольшой погрешностью.

- **HyperLogLog (HLL).**  
  Позволяет оценить число различных элементов в потоке данных при очень малой памяти (порядка килобайт/мегабайт).  
  Дает оценку с контролируемой относительной ошибкой (зависит от числа регистров).  
  **Минус:** результат приближенный

- **KMV / MinHash-подход (K minimum values).**  
  Хэшируем элементы и храним k наименьших хэшей. По распределению хэшей оцениваем мощность множества.  
  **Минус:** результат приближенный

**Почему в этом решении не используется вероятностный метод.**  
В задаче требуется **точное** количество уникальных IPv6-адресов. Вероятностные методы дают приближенный ответ или могут допускать ошибки, поэтому для гарантированной корректности выбран детерминированный алгоритм (внешняя сортировка + k-way merge).

---
